# main.py

import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
import os
from huggingface_hub import login

# Define the model path
model_name = "gpt2-medium"
model_path = "./admin_unit_model_gpt2_medium"

# Hugging Face username and token
hf_username = "thanhtunguet"  # Replace with your Hugging Face username
hf_token = os.environ.get("HUGGINGFACE_TOKEN")  # Ensure your token is set as an environment variable

if not hf_token:
    raise ValueError("Please set the HUGGINGFACE_TOKEN environment variable.")

# Log in to Hugging Face
login(token=hf_token)

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos_token to avoid warnings
model = AutoModelForCausalLM.from_pretrained(model_name)

# Load dataset from the JSON file generated by extract.py
dataset = load_dataset("json", data_files="data/vietnamese_administrative_units_train.json")

# Preprocessing function for tokenization
def preprocess_function(examples):
    inputs = examples["prompt"]
    targets = examples["completion"]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    # Tokenize targets and set as labels
    labels = tokenizer(targets, max_length=512, truncation=True, padding="max_length")["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs

# Tokenize dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Split dataset into training and evaluation
tokenized_dataset = tokenized_dataset["train"].train_test_split(test_size=0.1)
train_dataset = tokenized_dataset["train"]
eval_dataset = tokenized_dataset["test"]

# Set training arguments
training_args = TrainingArguments(
    output_dir=model_path,
    eval_strategy="epoch",  # Use 'eval_strategy' instead of 'evaluation_strategy'
    learning_rate=5e-5,
    per_device_train_batch_size=2,  # Adjust batch size if necessary
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    fp16=True,  # Enable mixed precision if supported
    push_to_hub=True,  # Enable pushing to Hugging Face Hub
    hub_model_id=f"{hf_username}/vietnamese-administrative-units-model",
    hub_token=hf_token,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    processing_class=tokenizer,
)

# Train the model
trainer.train()

# Save model and tokenizer locally
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

# Push the trained model to Hugging Face Hub
trainer.push_to_hub()

print(f"Training complete. Model saved to {model_path} and pushed to Hugging Face Hub.")
